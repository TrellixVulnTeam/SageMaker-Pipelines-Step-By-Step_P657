{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 3.1] 전처리 스텝 개발 (SageMaker Model Building Pipeline 전처리 스텝)\n",
    "\n",
    "이 노트북은 아래와 같은 목차로 진행 됩니다. 전체를 모두 실행시에 완료 시간은 약 5분-10분 소요 됩니다.\n",
    "\n",
    "- 0. 전처리 개요 (SageMaker Processing 이용) \n",
    "- 1. 기본 세이지 메이커 정보 및 변수 로딩\n",
    "- 2. 전처리 코드 확인\n",
    "- 3. 전치리 스텝 개발 및 실행\n",
    "    - 아래의 3단계를 진행하여 SageMaker Model Building Pipeline 에서 전치러 스텝 개발 함. 아래의 (1), (2) 단계는 옵션이지만, 실제 현업 개발시에 필요한 단계이기에 실행을 권장 드립니다.\n",
    "        - (1) [옵션] 로컬 노트북 인스턴스에서 **[다커 컨테이너 없이]** 전처리 코드 실행\n",
    "        - (2) [옵션] 로컬 노트북 인스턴스에서 **[다커 컨테이너를 통해서]** 전처리 코드 실행          \n",
    "        - (3) [필수] SageMaker Model Building Pipeline 에서 전치러 스텝 개발 및 실행\n",
    "    \n",
    "---\n",
    "### 노트북 커널\n",
    "- 이 워크샵은 노트북 커널이 `conda_python3` 를 사용합니다. 다른 커널일 경우 변경 해주세요.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 전처리 개요 (SageMaker Processing 이용)\n",
    "\n",
    "이 노트북은 세이지 메이커의 Processing Job을 통해서 데이터 전처리를 합니다. <br>\n",
    "상세한 사항은 개발자 가이드를 참조 하세요. -->  [SageMaker Processing](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/processing-job.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Processing 개요\n",
    "![sagemaker-processing-archi.png](img/sagemaker-processing-archi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 실행 요약\n",
    "![Processing-1.png](img/Processing-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 크게 아래 4가지의 스텝으로 진행이 됩니다.\n",
    "\n",
    "    - (1) S3에 입력 파일 준비\n",
    "    - (2) 전처리를 수행하는 코드 준비\n",
    "    - (3) Projcessing Job을 생성시에 아래와 같은 항목을 제공합니다.\n",
    "        - Projcessing Job을 실행할 EC2(예: ml.m4.2xlarge) 기술\n",
    "        - EC2에서 로딩할 다커 이미지의 이름 기술\n",
    "        - S3 입력 파일 경로\n",
    "        - 전처리 코드 경로\n",
    "        - S3 출력 파일 경로\n",
    "    - (4) EC2에서 전치리 실행 하여 S3 출력 위치에 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기본 세이지 메이커 정보 및 기본 변수 로딩\n",
    "- 기본으로 사용하는 세이지 메이커 변수의 로딩 및 `%store -r` 을 통해서 이전 노트북에서 저장한 변수를 로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from IPython.display import display as dp\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "%store -r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "claims_data_uri                     -> 's3://sagemaker-ap-northeast-2-057716757052/sagema\n",
      "customers_data_uri                  -> 's3://sagemaker-ap-northeast-2-057716757052/sagema\n",
      "default_bucket                      -> 'sagemaker-ap-northeast-2-057716757052'\n",
      "full_exp_pickle_file                -> 'preproc_data/full/full_exp.pkl'\n",
      "full_impute_pickle_file             -> 'preproc_data/full_imputtion.pkl'\n",
      "full_raw_pickle_file                -> 'preproc_data/full_raw.pkl'\n",
      "input_data_uri                      -> 's3://sagemaker-ap-northeast-2-057716757052/sagema\n",
      "model_artifact                      -> 's3://sagemaker-ap-northeast-2-057716757052/defaul\n",
      "prediction_csv_file                 -> 'report/raw/prediction_df.csv'\n",
      "preprocessing_code                  -> 'src/preprocessing.py'\n",
      "project_prefix                      -> 'sagemaker-pipeline-step-by-step'\n",
      "test_csv_file                       -> 'report/raw/test_df.csv'\n",
      "test_raw_pickle_file                -> 'preproc_data/test/test_raw.pkl'\n",
      "train_raw_pickle_file               -> 'preproc_data/train/train_raw.pkl'\n",
      "y_test_csv_file                     -> 'report/raw/ytest_df.csv'\n"
     ]
    }
   ],
   "source": [
    "# 현재 로컬 디스크에 저장되어 있는 변수를 확인\n",
    "%store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 전처리 코드 확인\n",
    "\n",
    "전처리 코드는 크게 아래와 같이 구성 되어 있습니다.\n",
    "- 커맨드 인자로 전달된 변수 내용 확인\n",
    "- 두개의 입력 파일(claims.csv, customers.csv)을 로딩하고, policy_id 로 조인하여 하나의 데이터 세트로 만듦\n",
    "- 카테고리 피쳐를 원핫인코딩을 함.\n",
    "- 숫자형 변수의 결측값에 대해서 채우는 전처리 수행\n",
    "    - [알림] 이 워크샵은 XGBoost를 통해서 훈련을 합니다. XGBoost 같은 Tree 계열의 알고리즘은 피쳐의 스케일링(Scaling)이 꼭 필요하지 않습니다.\n",
    "- 전치리 결과를 모두 결합 합니다.\n",
    "- 훈련, 테스트 데이터 세트로 split_rate (예: 8:2) 로 분리 합니다.\n",
    "    - [알림] 일반적으로 Fruad Detection의 훈련, 테스트 데이터의 분리는 시간의 순서에 따라 분리 합니다. 예를 들어서 2021.01.01 ~ 2021.12.31 의 데이터가 존재한다고 하면, 2021.01.01 ~ 2021.11.30 을 훈련 데이터, 2021.12.01 ~ 2021.12.31을 테스트 테이터로 분리 합니다. 이 워크샵에서 사용하고 있는 데이터 세트는 명시적인 날짜 정보가 없어서, 5000개의 의 policy_id에서 1~4,000 은 훈련 데이터, 4000 ~ 5,000은 테스트 데이터로 분리 하였습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'preprocessing_code' (str)\n"
     ]
    }
   ],
   "source": [
    "preprocessing_code = 'src/preprocessing.py'\n",
    "%store preprocessing_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtempfile\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m, \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcompose\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ColumnTransformer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mimpute\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SimpleImputer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpipeline\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StandardScaler, OneHotEncoder\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mhandlers\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_logger\u001b[39;49;00m():\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    로깅을 위해 파이썬 로거를 사용\u001b[39;49;00m\n",
      "\u001b[33m    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    loglevel = logging.DEBUG\n",
      "    l = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m l.hasHandlers():\n",
      "        l.setLevel(loglevel)\n",
      "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
      "        l.handler_set = \u001b[34mTrue\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m l  \n",
      "\n",
      "logger = _get_logger()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msplit_train_test\u001b[39;49;00m(df, test_ratio=\u001b[34m0.1\u001b[39;49;00m):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    두 개의 데이터 세트로 분리\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    total_rows = df.shape[\u001b[34m0\u001b[39;49;00m]\n",
      "    train_end = \u001b[36mint\u001b[39;49;00m(total_rows * (\u001b[34m1\u001b[39;49;00m - test_ratio))\n",
      "    \n",
      "    train_df = df[\u001b[34m0\u001b[39;49;00m:train_end]\n",
      "    test_df = df[train_end:]\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m train_df, test_df\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_dataframe\u001b[39;49;00m(base_preproc_input_dir, file_name_prefix ):    \n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \n",
      "    input_files = glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m*.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(base_preproc_input_dir, file_name_prefix))\n",
      "    \u001b[37m#claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33minput_files: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_files\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_files) == \u001b[34m0\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m((\u001b[33m'\u001b[39;49;00m\u001b[33mThere are no files in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\n",
      "                          \u001b[33m'\u001b[39;49;00m\u001b[33mThis usually indicates that the channel (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m) was incorrectly specified,\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\n",
      "                          \u001b[33m'\u001b[39;49;00m\u001b[33mthe data specification in S3 was incorrectly specified or the role specified\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\n",
      "                          \u001b[33m'\u001b[39;49;00m\u001b[33mdoes not have permission to access the data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).format(base_preproc_input_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "        \n",
      "    raw_data = [ pd.read_csv(file, index_col=\u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m input_files ]\n",
      "    df = pd.concat(raw_data)\n",
      "   \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdataframe shape \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdf.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdataset sample \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdf.head(\u001b[34m2\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)        \n",
      "    \u001b[37m#logger.info(f\"df columns \\n {df.columns}\")    \u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_type\u001b[39;49;00m(raw, cols, type_target):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    해당 데이터 타입으로 변경\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    df = raw.copy()\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m col \u001b[35min\u001b[39;49;00m cols:\n",
      "        df[col] = df[col].astype(type_target)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m df\n",
      "    \n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m ==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m################################\u001b[39;49;00m\n",
      "    \u001b[37m#### 커맨드 인자 파싱   \u001b[39;49;00m\n",
      "    \u001b[37m#################################        \u001b[39;49;00m\n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--base_output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--base_preproc_input_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)   \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--split_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m)       \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--label_column\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfraud\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)       \n",
      "    \u001b[37m# parse arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()     \n",
      "    \n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33m######### Argument Info ####################################\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33margs.base_output_dir: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.base_output_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33margs.base_preproc_input_dir: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.base_preproc_input_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33margs.label_column: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.label_column\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)        \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33margs.split_rate: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.split_rate\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)            \n",
      "\n",
      "    base_output_dir = args.base_output_dir\n",
      "    base_preproc_input_dir = args.base_preproc_input_dir\n",
      "    label_column = args.label_column    \n",
      "    split_rate = args.split_rate\n",
      "\n",
      "    \u001b[37m#################################        \u001b[39;49;00m\n",
      "    \u001b[37m#### 두개의 파일(claim, customer) 을 로딩하여 policy_id 로 조인함  ########\u001b[39;49;00m\n",
      "    \u001b[37m#################################    \u001b[39;49;00m\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m### Loading Claim Dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    claim_df = get_dataframe(base_preproc_input_dir,file_name_prefix=\u001b[33m'\u001b[39;49;00m\u001b[33mclaim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m )        \n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m### Loading Customer Dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    customer_df = get_dataframe(base_preproc_input_dir,file_name_prefix=\u001b[33m'\u001b[39;49;00m\u001b[33mcustomer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m )            \n",
      "    \n",
      "    df = customer_df.join(claim_df, how=\u001b[33m'\u001b[39;49;00m\u001b[33mleft\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m### dataframe merged with customer and claim: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdf.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "    \u001b[37m#################################    \u001b[39;49;00m\n",
      "    \u001b[37m#### 카테고리 피쳐를 원핫인코딩  \u001b[39;49;00m\n",
      "    \u001b[37m#################################    \u001b[39;49;00m\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m ### Encoding: Category Features\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    categorical_features = df.select_dtypes(include=[\u001b[33m'\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).columns.values.tolist()    \n",
      "    \u001b[37m#categorical_features = ['driver_relationship']    \u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mcategorical_features: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcategorical_features\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)            \n",
      "\n",
      "    categorical_transformer = Pipeline(\n",
      "        steps=[\n",
      "            (\u001b[33m\"\u001b[39;49;00m\u001b[33mimputer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SimpleImputer(strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mconstant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, fill_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mmissing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)),\n",
      "            (\u001b[33m\"\u001b[39;49;00m\u001b[33monehot\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OneHotEncoder(handle_unknown=\u001b[33m\"\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "        ]\n",
      "    )\n",
      "    \n",
      "    preprocess = ColumnTransformer(\n",
      "        transformers=[\n",
      "            (\u001b[33m\"\u001b[39;49;00m\u001b[33mcat\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, categorical_transformer, categorical_features)\n",
      "        ],\n",
      "        sparse_threshold = \u001b[34m0\u001b[39;49;00m, \u001b[37m# dense format 으로 제공\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    X_pre_category = preprocess.fit_transform(df)\n",
      "    \n",
      "\n",
      "    \u001b[37m# 원핫인코딩한 컬럼의 이름 로딩\u001b[39;49;00m\n",
      "    \u001b[37m# Ref: Sklearn Pipeline: Get feature names after OneHotEncode In ColumnTransformer,  https://stackoverflow.com/questions/54646709/sklearn-pipeline-get-feature-names-after-onehotencode-in-columntransformer\u001b[39;49;00m\n",
      "    \n",
      "    processed_category_features = preprocess.transformers_[\u001b[34m0\u001b[39;49;00m][\u001b[34m1\u001b[39;49;00m].named_steps[\u001b[33m'\u001b[39;49;00m\u001b[33monehot\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].get_feature_names(categorical_features)\n",
      "    \u001b[37m#logger.info(f\"processed_category_features: {processed_category_features}\")\u001b[39;49;00m\n",
      "\u001b[37m#    print(X_pre)\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m###############################\u001b[39;49;00m\n",
      "    \u001b[37m### 숫자형 변수 전처리 \u001b[39;49;00m\n",
      "    \u001b[37m###############################\u001b[39;49;00m\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m ### Encoding: Numeric Features\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)        \n",
      "    \n",
      "    float_cols = df.select_dtypes(include=[\u001b[33m'\u001b[39;49;00m\u001b[33mfloat64\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).columns.values\n",
      "    int_cols = df.select_dtypes(include=[\u001b[33m'\u001b[39;49;00m\u001b[33mint64\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).columns.values\n",
      "    numeric_features = np.concatenate((float_cols, int_cols), axis=\u001b[34m0\u001b[39;49;00m).tolist()\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mint_cols: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mint_cols\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat_cols: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mfloat_cols\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)        \n",
      "    \u001b[37m#logger.info(f\"numeric_features: \\n{numeric_features}\")\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# 따로 스케일링은 하지 않고, 미싱 값만 중간값을 취함\u001b[39;49;00m\n",
      "    numeric_transformer = Pipeline(\n",
      "        steps=[\n",
      "            (\u001b[33m\"\u001b[39;49;00m\u001b[33mimputer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SimpleImputer(strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mmedian\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)),\n",
      "           \u001b[37m# (\"scaler\", StandardScaler())\u001b[39;49;00m\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    numeric_preprocessor = ColumnTransformer(\n",
      "        transformers=[\n",
      "            (\u001b[33m\"\u001b[39;49;00m\u001b[33mcat\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, numeric_transformer, numeric_features)\n",
      "        ],\n",
      "        sparse_threshold = \u001b[34m0\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    X_pre_numeric = numeric_preprocessor.fit_transform(df)    \n",
      "\n",
      "    \n",
      "    \u001b[37m###############################\u001b[39;49;00m\n",
      "    \u001b[37m### 전처리 결과 결합 ####\u001b[39;49;00m\n",
      "    \u001b[37m###############################\u001b[39;49;00m\n",
      "    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m ### Handle preprocess results\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)            \n",
      "    \n",
      "    \u001b[37m# 전처리 결과를 데이터 프레임으로 생성\u001b[39;49;00m\n",
      "    category_df = pd.DataFrame(data=X_pre_category, columns=processed_category_features)\n",
      "    numeric_df = pd.DataFrame(data=X_pre_numeric, columns=numeric_features)    \n",
      "\n",
      "    full_df = pd.concat([numeric_df, category_df ], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# float 타입을 int 로 변경\u001b[39;49;00m\n",
      "    full_df = convert_type(full_df, cols=int_cols, type_target=\u001b[33m'\u001b[39;49;00m\u001b[33mint\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    full_df = convert_type(full_df, cols=processed_category_features, type_target=\u001b[33m'\u001b[39;49;00m\u001b[33mint\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \n",
      "    \n",
      "    \u001b[37m# label_column을 맨 앞으로 이동 시킴\u001b[39;49;00m\n",
      "    full_df = pd.concat([full_df[label_column], full_df.drop(columns=[label_column])], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m###############################    \u001b[39;49;00m\n",
      "    \u001b[37m# 훈련, 테스트 데이터 세트로 분리 및 저장\u001b[39;49;00m\n",
      "    \u001b[37m###############################\u001b[39;49;00m\n",
      "    \n",
      "    train_df, test_df = split_train_test(full_df, test_ratio=split_rate)    \n",
      "    train_df.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_output_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/train/train.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    test_df.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_output_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/test/test.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpreprocessed train shape \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_df.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)        \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpreprocessed validation shape \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_df.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)            \n",
      "\n",
      "    \u001b[37m# logger.info(f\"preprocessed train path \\n {base_output_dir}/train/train.csv\")\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m ### Final result for train dataset \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpreprocessed train sample \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_df.head(\u001b[34m2\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!pygmentize {preprocessing_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 전치리 스텝 개발 및 실행\n",
    "- 전처리 스텝의 개발이 현업에서 아래의 (1), (2), (3) 의 순서로 개발이 될 수 있습니다. 그래서 각 단계별로 실행을 진행 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 개요\n",
    "### (1) 로컬 노트북 인스턴스에서 **[다커 컨테이너 없이]** 전처리 코드 실행\n",
    "\n",
    "- 이 단계의 목적은 현업 프로젝트의 개발시에 로직 확인, 디버깅이 수월하기 위해서 이 단계를 진행 합니다. \n",
    "- SageMaker Processing은 다커 컨테이너에서 실행이 되기에, 로컬에서 다커 환경과 비슷한 환경을 구성(예: 입력, 출력 위치)하여 실행합니다.\n",
    "\n",
    "\n",
    "### (2) 로컬 노트북 인스턴스에서 **[다커 컨테이너를 통해서]** 전처리 코드 실행      \n",
    "- 이 단계의 목적은 SageMaker Model Building Pipeline이 다커 컨테이너 형태로 실행이 되기에, 이 단계에서 다커 컨테이너에서 정상적으로 동작하는지를 확인하기 위해서 이 단계를 수행 합니다.\n",
    "- 위의 (1) 단계에서 전처리 코드의 로직 확인이 되었기에, 실제 로컬 노트북 인스턴스에서 다커 컨테이너를 통해 전치리 코드를 수행 합니다.\n",
    "- [알림] 로컬 모드 및 다커 참고 자료\n",
    "    - 로컬모드 설명하는 블로그 자료 --> [Use the Amazon SageMaker local mode to train on your notebook instance](https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/)\n",
    "    - TF, Pytorch, SKLean, SKLearn Processing JOb에 대한 로컬 모드 샘플 --> [Amazon SageMaker Local Mode Examples](https://github.com/aws-samples/amazon-sagemaker-local-mode)\n",
    "    - Python SDK -->  [로컬모드 Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode)\n",
    "    - [SageMaker 에서 도커 컨테이너 사용하기](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/docker-containers.html)\n",
    "    - [Hello 다커](https://github.com/mullue/hello-docker/blob/master/hello_docker.ipynb)\n",
    "    \n",
    "### (3) SageMaker Model Building Pipeline 에서 전치러 스텝 개발 및 실행\n",
    "- 상세 사항은 여기에서 확인 하세요. --> [Amazon SageMaker 모델 구축 파이프라인](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/pipelines.html)\n",
    "    \n",
    "---    \n",
    "## 3.2 실행    \n",
    "### (1) 로컬 노트북 인스턴스에서 **[다커 컨테이너 없이]** 전처리 코드 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 환경 셋업 \n",
    "\n",
    "- 로컬에서 테스트 하기 위해 세이지메이커의 다커 컨테이너와 같은 환경을 생성합니다.\n",
    "- split_rate = 0.2 로 해서 훈련 및 테스트 데이터 세트의 비율을 8:2로 정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 도커 컨테이너의 출력 폴더와 비슷한 환경 기술\n",
    "# 아래 경로 : opt/ml/processing/output\n",
    "# 도커 경로 : /opt/ml/processing/output\n",
    "base_output_dir = 'opt/ml/processing/output' \n",
    "\n",
    "# 도커 컨테이너의 입력 폴더와 비슷한 환경 기술\n",
    "base_preproc_input_dir = 'opt/ml/processing/input'\n",
    "os.makedirs(base_preproc_input_dir, exist_ok=True)\n",
    "\n",
    "# 출력 훈련 폴더를 기술 합니다.\n",
    "base_preproc_output_train_dir = 'opt/ml/processing/output/train/'\n",
    "os.makedirs(base_preproc_output_train_dir, exist_ok=True)\n",
    "\n",
    "# 출력 테스트 폴더를 기술 합니다.\n",
    "base_preproc_output_test_dir = 'opt/ml/processing/output/test/'\n",
    "os.makedirs(base_preproc_output_test_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "split_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### claims.csv, customers.csv 를 다커환경에서 사용하는 입력 경로로 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../data/claims.csv {base_preproc_input_dir}\n",
    "! cp ../data/customers.csv {base_preproc_input_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬에서 스크립트 실행\n",
    "- 전처리 코드에서 제공하는 로그를 통해서, 전처리 수행 내역을 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Argument Info ####################################\n",
      "args.base_output_dir: opt/ml/processing/output\n",
      "args.base_preproc_input_dir: opt/ml/processing/input\n",
      "args.label_column: fraud\n",
      "args.split_rate: 0.2\n",
      "\n",
      "### Loading Claim Dataset\n",
      "input_files: \n",
      " ['opt/ml/processing/input/claims.csv']\n",
      "dataframe shape \n",
      " (5000, 17)\n",
      "dataset sample \n",
      "           driver_relationship incident_type  ... incident_hour fraud\n",
      "policy_id                                    ...                    \n",
      "1                      Spouse     Collision  ...             8     0\n",
      "2                        Self     Collision  ...            11     0\n",
      "\n",
      "[2 rows x 17 columns]\n",
      "\n",
      "### Loading Customer Dataset\n",
      "input_files: \n",
      " ['opt/ml/processing/input/customers.csv']\n",
      "dataframe shape \n",
      " (5000, 12)\n",
      "dataset sample \n",
      "            customer_age  months_as_customer  ...  customer_education  auto_year\n",
      "policy_id                                    ...                               \n",
      "1                    54                  94  ...           Associate       2006\n",
      "2                    41                 165  ...            Bachelor       2012\n",
      "\n",
      "[2 rows x 12 columns]\n",
      "### dataframe merged with customer and claim: (5000, 29)\n",
      "\n",
      " ### Encoding: Category Features\n",
      "categorical_features: ['policy_state', 'policy_liability', 'customer_gender', 'customer_education', 'driver_relationship', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'police_report_available']\n",
      "\n",
      " ### Encoding: Numeric Features\n",
      "int_cols: \n",
      "['customer_age' 'months_as_customer' 'num_claims_past_year'\n",
      " 'num_insurers_past_5_years' 'policy_deductable' 'policy_annual_premium'\n",
      " 'customer_zip' 'auto_year' 'num_vehicles_involved' 'num_injuries'\n",
      " 'num_witnesses' 'injury_claim' 'incident_month' 'incident_day'\n",
      " 'incident_dow' 'incident_hour' 'fraud']\n",
      "float_cols: \n",
      "['vehicle_claim' 'total_claim_amount']\n",
      "\n",
      " ### Handle preprocess results\n",
      "preprocessed train shape \n",
      " (4000, 59)\n",
      "preprocessed validation shape \n",
      " (1000, 59)\n",
      "\n",
      " ### Final result for train dataset \n",
      "preprocessed train sample \n",
      "    fraud  ...  police_report_available_Yes\n",
      "0      0  ...                            0\n",
      "1      0  ...                            1\n",
      "\n",
      "[2 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "! python {preprocessing_code} --base_preproc_input_dir {base_preproc_input_dir} \\\n",
    "                              --base_output_dir {base_output_dir} \\\n",
    "                              --split_rate {split_rate}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리된 데이터 확인\n",
    "- 실제로 전처리 된 파일의 내역을 확인 합니다.\n",
    "- 훈련 및 테스트 세트의 fraud 의 분포를 확인 합니다. (0: non-fruad, 1: fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>num_claims_past_year</th>\n",
       "      <th>num_insurers_past_5_years</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>customer_zip</th>\n",
       "      <th>...</th>\n",
       "      <th>collision_type_missing</th>\n",
       "      <th>incident_severity_Major</th>\n",
       "      <th>incident_severity_Minor</th>\n",
       "      <th>incident_severity_Totaled</th>\n",
       "      <th>authorities_contacted_Ambulance</th>\n",
       "      <th>authorities_contacted_Fire</th>\n",
       "      <th>authorities_contacted_None</th>\n",
       "      <th>authorities_contacted_Police</th>\n",
       "      <th>police_report_available_No</th>\n",
       "      <th>police_report_available_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8913.668763</td>\n",
       "      <td>80513.668763</td>\n",
       "      <td>54</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>99207</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>19746.724395</td>\n",
       "      <td>26146.724395</td>\n",
       "      <td>41</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>2950</td>\n",
       "      <td>95632</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11652.969918</td>\n",
       "      <td>22052.969918</td>\n",
       "      <td>57</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>93203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11260.930936</td>\n",
       "      <td>115960.930936</td>\n",
       "      <td>39</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>85208</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>27987.704652</td>\n",
       "      <td>31387.704652</td>\n",
       "      <td>39</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>91792</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0</td>\n",
       "      <td>18052.611626</td>\n",
       "      <td>67152.611626</td>\n",
       "      <td>42</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>93654</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>34949.202468</td>\n",
       "      <td>51749.202468</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>94305</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0</td>\n",
       "      <td>4063.701410</td>\n",
       "      <td>9963.701410</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>750</td>\n",
       "      <td>2550</td>\n",
       "      <td>95476</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0</td>\n",
       "      <td>17390.520451</td>\n",
       "      <td>20490.520451</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>90680</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0</td>\n",
       "      <td>2501.811593</td>\n",
       "      <td>8401.811593</td>\n",
       "      <td>57</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "      <td>2650</td>\n",
       "      <td>98029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fraud  vehicle_claim  total_claim_amount  customer_age  \\\n",
       "0         0    8913.668763        80513.668763            54   \n",
       "1         0   19746.724395        26146.724395            41   \n",
       "2         0   11652.969918        22052.969918            57   \n",
       "3         0   11260.930936       115960.930936            39   \n",
       "4         0   27987.704652        31387.704652            39   \n",
       "...     ...            ...                 ...           ...   \n",
       "3995      0   18052.611626        67152.611626            42   \n",
       "3996      0   34949.202468        51749.202468            23   \n",
       "3997      0    4063.701410         9963.701410            44   \n",
       "3998      0   17390.520451        20490.520451            22   \n",
       "3999      0    2501.811593         8401.811593            57   \n",
       "\n",
       "      months_as_customer  num_claims_past_year  num_insurers_past_5_years  \\\n",
       "0                     94                     0                          1   \n",
       "1                    165                     0                          1   \n",
       "2                    155                     0                          1   \n",
       "3                     80                     0                          1   \n",
       "4                     60                     0                          1   \n",
       "...                  ...                   ...                        ...   \n",
       "3995                 103                     1                          1   \n",
       "3996                   6                     0                          3   \n",
       "3997                  35                     0                          2   \n",
       "3998                  38                     0                          1   \n",
       "3999                  74                     0                          1   \n",
       "\n",
       "      policy_deductable  policy_annual_premium  customer_zip  ...  \\\n",
       "0                   750                   3000         99207  ...   \n",
       "1                   750                   2950         95632  ...   \n",
       "2                   750                   3000         93203  ...   \n",
       "3                   750                   3000         85208  ...   \n",
       "4                   750                   3000         91792  ...   \n",
       "...                 ...                    ...           ...  ...   \n",
       "3995                750                   3000         93654  ...   \n",
       "3996                750                   3000         94305  ...   \n",
       "3997                750                   2550         95476  ...   \n",
       "3998                750                   3000         90680  ...   \n",
       "3999                900                   2650         98029  ...   \n",
       "\n",
       "      collision_type_missing  incident_severity_Major  \\\n",
       "0                          0                        0   \n",
       "1                          0                        0   \n",
       "2                          0                        0   \n",
       "3                          0                        0   \n",
       "4                          0                        1   \n",
       "...                      ...                      ...   \n",
       "3995                       0                        0   \n",
       "3996                       0                        0   \n",
       "3997                       0                        0   \n",
       "3998                       0                        1   \n",
       "3999                       0                        1   \n",
       "\n",
       "      incident_severity_Minor  incident_severity_Totaled  \\\n",
       "0                           1                          0   \n",
       "1                           0                          1   \n",
       "2                           1                          0   \n",
       "3                           1                          0   \n",
       "4                           0                          0   \n",
       "...                       ...                        ...   \n",
       "3995                        1                          0   \n",
       "3996                        0                          1   \n",
       "3997                        1                          0   \n",
       "3998                        0                          0   \n",
       "3999                        0                          0   \n",
       "\n",
       "      authorities_contacted_Ambulance  authorities_contacted_Fire  \\\n",
       "0                                   0                           0   \n",
       "1                                   0                           0   \n",
       "2                                   0                           0   \n",
       "3                                   0                           0   \n",
       "4                                   0                           0   \n",
       "...                               ...                         ...   \n",
       "3995                                0                           0   \n",
       "3996                                1                           0   \n",
       "3997                                0                           0   \n",
       "3998                                0                           0   \n",
       "3999                                0                           0   \n",
       "\n",
       "      authorities_contacted_None  authorities_contacted_Police  \\\n",
       "0                              1                             0   \n",
       "1                              0                             1   \n",
       "2                              0                             1   \n",
       "3                              1                             0   \n",
       "4                              0                             1   \n",
       "...                          ...                           ...   \n",
       "3995                           1                             0   \n",
       "3996                           0                             0   \n",
       "3997                           0                             1   \n",
       "3998                           0                             1   \n",
       "3999                           0                             1   \n",
       "\n",
       "      police_report_available_No  police_report_available_Yes  \n",
       "0                              1                            0  \n",
       "1                              0                            1  \n",
       "2                              0                            1  \n",
       "3                              1                            0  \n",
       "4                              1                            0  \n",
       "...                          ...                          ...  \n",
       "3995                           1                            0  \n",
       "3996                           1                            0  \n",
       "3997                           0                            1  \n",
       "3998                           0                            1  \n",
       "3999                           0                            1  \n",
       "\n",
       "[4000 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_path = os.path.join(base_output_dir + '/train/train.csv')\n",
    "preprocessed_test_path = os.path.join(base_output_dir + '/test/test.csv')\n",
    "\n",
    "preprocessed_train_df = pd.read_csv(preprocessed_train_path)\n",
    "preprocessed_test_df = pd.read_csv(preprocessed_test_path)\n",
    "\n",
    "preprocessed_train_df\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번때 데이터 세트는 훈련 데이터 세트, 두번째는 테스트 데이터 세트 입니다. 각각의 fraud의 비율을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Set: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fraud\n",
       "0        3869\n",
       "1         131\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data Set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fraud\n",
       "0        967\n",
       "1         33\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Train Data Set: \")\n",
    "dp(preprocessed_train_df[['fraud']].value_counts())\n",
    "\n",
    "print(\"\\nTest Data Set:\")\n",
    "dp(preprocessed_test_df[['fraud']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 로컬 노트북 인스턴스에서 **[다커 컨테이너를 통해서]** 전처리 코드 실행      \n",
    "- 아래 셀은 최초 실행시에 약 1분-2분 정도 소요 됩니다. 이후에는 약 5초 정도 걸립니다.\n",
    "    - 최초 실행에는 SKLearnProcessor 의 다커 이미지를 [AWS ECR](https://aws.amazon.com/ko/ecr/) 에서 다운로드 받기에 시간이 걸립니다.\n",
    "    - 아래와 같이 터미널에서 명령어를 실행하면 도커 이미지가 다운로드가 된 것을 확인할 수 있습니다.\n",
    "    \n",
    "```\n",
    "sh-4.2$ docker image ls\n",
    "REPOSITORY                                                                 TAG                 IMAGE ID            CREATED             SIZE\n",
    "366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn   0.23-1-cpu-py3      17d4d9baed63        4 months ago        3.36GB\n",
    "```\n",
    "\n",
    "- instance_type 을 local 로 해야만, 로컬 모드로 수행 됩니다.\n",
    "    - [알림] `local` 은 CPU를 이용한다는 의미이고, 참고로 현재 노트북 인스턴스에 GPU가 있으면 'local_gpu' 기술하여 GPU를 사용할 수 있습니다.\n",
    "- 아래 코드의 상세 설명은 여기를 참조 하세요. --> [Scikit-learn을 통한 데이터 처리](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/use-scikit-learn-processing-container.html)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-08-02-14-20-00-382\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sagemaker-pipeline-step-by-step/input', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sagemaker-scikit-learn-2021-08-02-14-20-00-382/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sagemaker-scikit-learn-2021-08-02-14-20-00-382/output/output-1', 'LocalPath': '/opt/ml/processing/output/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-2', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sagemaker-scikit-learn-2021-08-02-14-20-00-382/output/output-2', 'LocalPath': '/opt/ml/processing/output/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      "Creating mnf250x69g-algo-1-h7kyc ... \n",
      "Creating mnf250x69g-algo-1-h7kyc ... done\n",
      "Attaching to mnf250x69g-algo-1-h7kyc\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m ######### Argument Info ####################################\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m args.base_output_dir: /opt/ml/processing/output\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m args.base_preproc_input_dir: /opt/ml/processing/input\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m args.label_column: fraud\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m args.split_rate: 0.2\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m ### Loading Claim Dataset\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m input_files: \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  ['/opt/ml/processing/input/claims.csv']\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m dataframe shape \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  (5000, 17)\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m dataset sample \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m            driver_relationship  ... fraud\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m policy_id                      ...      \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m 1                      Spouse  ...     0\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m 2                        Self  ...     0\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m [2 rows x 17 columns]\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m ### Loading Customer Dataset\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m input_files: \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  ['/opt/ml/processing/input/customers.csv']\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m dataframe shape \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  (5000, 12)\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m dataset sample \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m             customer_age  ...  auto_year\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m policy_id                ...           \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m 1                    54  ...       2006\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m 2                    41  ...       2012\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m [2 rows x 12 columns]\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m ### dataframe merged with customer and claim: (5000, 29)\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  ### Encoding: Category Features\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m categorical_features: ['policy_state', 'policy_liability', 'customer_gender', 'customer_education', 'driver_relationship', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'police_report_available']\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  ### Encoding: Numeric Features\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m int_cols: \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m ['customer_age' 'months_as_customer' 'num_claims_past_year'\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  'num_insurers_past_5_years' 'policy_deductable' 'policy_annual_premium'\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  'customer_zip' 'auto_year' 'num_vehicles_involved' 'num_injuries'\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  'num_witnesses' 'injury_claim' 'incident_month' 'incident_day'\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  'incident_dow' 'incident_hour' 'fraud']\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m float_cols: \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m ['vehicle_claim' 'total_claim_amount']\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  ### Handle preprocess results\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m preprocessed train shape \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  (4000, 59)\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m preprocessed validation shape \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  (1000, 59)\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m  ### Final result for train dataset \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m preprocessed train sample \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m     fraud  ...  police_report_available_Yes\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m 0      0  ...                            0\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m 1      0  ...                            1\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m \n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc |\u001b[0m [2 rows x 59 columns]\n",
      "\u001b[36mmnf250x69g-algo-1-h7kyc exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n",
      "."
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "instance_type = 'local'\n",
    "sklearn_processor = SKLearnProcessor(framework_version= \"0.23-1\",\n",
    "                                     role=role,\n",
    "                                     instance_type= instance_type,\n",
    "                                     instance_count=1,\n",
    "                                    )\n",
    "\n",
    "sklearn_processor.run(code= preprocessing_code,\n",
    "                      inputs=[\n",
    "                        ProcessingInput(source=input_data_uri,\n",
    "                                        destination='/opt/ml/processing/input'),\n",
    "                             ],\n",
    "                      outputs=[ProcessingOutput(source='/opt/ml/processing/output/train'),\n",
    "                               ProcessingOutput(source='/opt/ml/processing/output/test')],\n",
    "                      arguments=[\"--split_rate\", f\"{split_rate}\"], # 훈련 및 테스트의 비율을 인자로 제공\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) SageMaker Model Building Pipeline 에서 전치러 스텝 개발 및 실행\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 빌딩 파이프라인 변수 생성\n",
    "- 아래와 같이 변수를 정의하여, 파이프라인 정의시에 인자로 제공할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 스텝 프로세서 정의\n",
    "- 전처리의 내장 SKLearnProcessor 를 통해서 sklearn_processor 오브젝트를 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data: \n",
      " s3://sagemaker-ap-northeast-2-057716757052/sagemaker-pipeline-step-by-step/input\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-fraud-process\",\n",
    "    role=role,\n",
    ")\n",
    "print(\"input_data: \\n\", input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 스텝 단계 정의\n",
    "- 처리 단계에서는 아래와 같은 주요 인자가 있습니다.\n",
    "    - 단계 이름\n",
    "    - processor 기술: 위에서 생성한 processor 오브젝트를 제공\n",
    "    - inputs: S3의 경로를 기술하고, 다커안에서의 다운로드 폴더(destination)을 기술 합니다.\n",
    "    - outputs: 처리 결과가 저장될 다커안에서의 폴더 경로를 기술합니다.\n",
    "        - 다커안의 결과 파일이 저장 후에 자동으로 S3로 업로딩을 합니다.\n",
    "    - job_arguments: 사용자 정의의 인자를 기술 합니다.\n",
    "    - code: 전처리 코드의 경로를 기술 합니다.\n",
    "- 처리 단계의 상세한 사항은 여기를 보세요. --> [처리 단계, Processing Step](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"FraudScratchProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data_uri,destination='/opt/ml/processing/input'),\n",
    "         ],\n",
    "    outputs=[ProcessingOutput(output_name=\"train\",\n",
    "                              source='/opt/ml/processing/output/train'),\n",
    "             ProcessingOutput(output_name=\"test\",\n",
    "                              source='/opt/ml/processing/output/test')],\n",
    "    job_arguments=[\"--split_rate\", f\"{split_rate}\"],    \n",
    "    code= preprocessing_code\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파리마터, 단계, 조건을 조합하여 최종 파이프라인 정의\n",
    "- 파이프라인의 정의를 생성 합니다.\n",
    "    - name: 파이프라인의 이름\n",
    "    - parameters: 위에서 기술한 parameters를 기술 합니다.\n",
    "    - steps: 위에서 정의한 처리 스텝을 기술합니다.\n",
    "        - [알림] 정의한 stpes 이 복수개이면 복수개를 기술합니다. 만약에 step 간에 의존성이 있으면, 명시적으로 기술하지 않아도 같이 실행 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = project_prefix\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_process],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (선택) 파이프라인 정의 확인 \n",
    "\n",
    "파이프라인을 정의하는 JSON을 생성하고 파이프라인 내에서 사용하는 파라미터와 단계별 속성들이 잘 정의되었는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-ap-northeast-2-057716757052/sagemaker-pipeline-step-by-step/input'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'FraudScratchProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--split_rate', '0.2'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::057716757052:role/service-role/AmazonSageMaker-ExecutionRole-20210120T193680',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sagemaker-pipeline-step-by-step/input',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sklearn-fraud-process-2021-08-02-14-20-04-608/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sklearn-fraud-process-2021-08-02-14-20-04-608/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/output/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-057716757052/sklearn-fraud-process-2021-08-02-14-20-04-608/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/output/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이프라인을 SageMaker에 제출하고 실행하기 \n",
    "\n",
    "파이프라인 정의를 SageMaker Pipelines 서비스에 제출하여 파이프라인을 생성하거나 파이프라인이 이미 존재하면 파이프라인 정의를 업데이트합니다. 함께 전달되는 역할(role)을 이용하여 AWS에서 파이프라인을 생성하고 작업의 각 단계를 실행할 것입니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:057716757052:pipeline/sagemaker-pipeline-step-by-step',\n",
       " 'ResponseMetadata': {'RequestId': '7af809f7-a7ef-452f-bae0-0869f5b3ec08',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7af809f7-a7ef-452f-bae0-0869f5b3ec08',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '104',\n",
       "   'date': 'Mon, 02 Aug 2021 14:20:05 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디폴트값을 이용하여 파이프라인을 샐행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이프라인 운영: 파이프라인 대기 및 실행상태 확인\n",
    "\n",
    "워크플로우의 실행상황을 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행이 완료될 때까지 기다립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행된 단계들을 리스트업합니다. 파이프라인의 단계실행 서비스에 의해 시작되거나 완료된 단계를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'FraudScratchProcess',\n",
       "  'StartTime': datetime.datetime(2021, 8, 2, 14, 20, 7, 168000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2021, 8, 2, 14, 24, 11, 302000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:057716757052:processing-job/pipelines-bdqe0u3x00go-fraudscratchprocess-incsszklky'}}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [옵션] SageMaker Studio에서 확인하기\n",
    "- 아래의 그림 처럼 SageMaker Studio에 로긴후에 따라하시면, SageMaker Studio 에서도 실행 내역을 확인할 수 있습니다.\n",
    "- [알림] 이번 실습은 SageMaker Studio 와 병행하여 진행하는 것을 권장 드립니다. 하지만, SageMaker Studio 에서 다커 컨테이너를 실행하는(로컬 모드) 부분은 현재 기능이 없기에 SageMaker Notebook Instance를 사용해서 실습했습니다. 다커 컨테이너의 실행 부분을 생략한다면 SageMaker Studio에서 SageMaker Building Pipeline 의 작업을 권장 드립니다. 이유는 가시적으로 워크플로를 확인하고, GUI 에서 제공하는 여러 링크들이 작업을 하는데에 효과적이기 때문입니다.\n",
    "- SageMaker Studio 개발자 가이드 --> [SageMaker Studio](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/studio.html)\n",
    "![view_studio.png](img/view_studio.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아티펙트 경로 추출\n",
    "- 다음 노트북에서 사용할 훈련 및 테스트의 전처리 S3 경로를 저장 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_preproc_dir_artifact:  s3://sagemaker-ap-northeast-2-057716757052/sklearn-fraud-process-2021-08-02-14-20-04-608/output/train\n",
      "test_preproc__dir_artifact:  s3://sagemaker-ap-northeast-2-057716757052/sklearn-fraud-process-2021-08-02-14-20-04-608/output/test\n",
      "Stored 'train_preproc_dir_artifact' (str)\n",
      "Stored 'test_preproc_dir_artifact' (str)\n"
     ]
    }
   ],
   "source": [
    "def get_proc_artifact(execution, client, kind=0):\n",
    "    '''\n",
    "    kind: 0 --> train\n",
    "    kind: 2 --> test\n",
    "    '''\n",
    "    response = execution.list_steps()\n",
    "    proc_arn = response[0]['Metadata']['ProcessingJob']['Arn']\n",
    "    proc_job_name = proc_arn.split('/')[-1]\n",
    "    # print(\"proc_job_name: \", proc_job_name)\n",
    "    \n",
    "    response = client.describe_processing_job(ProcessingJobName = proc_job_name)\n",
    "    train_preproc_artifact = response['ProcessingOutputConfig']['Outputs'][kind]['S3Output']['S3Uri']    \n",
    "    \n",
    "    return train_preproc_artifact\n",
    "\n",
    "import boto3\n",
    "client = boto3.client(\"sagemaker\")\n",
    "\n",
    "train_preproc_dir_artifact = get_proc_artifact(execution, client, kind=0 )\n",
    "test_preproc_dir_artifact = get_proc_artifact(execution, client, kind=1 )\n",
    "\n",
    "print(\"train_preproc_dir_artifact: \", train_preproc_dir_artifact)\n",
    "print(\"test_preproc__dir_artifact: \", test_preproc_dir_artifact)\n",
    "\n",
    "%store train_preproc_dir_artifact\n",
    "%store test_preproc_dir_artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아티펙트의 S3 저장 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-02 14:24:05     682602 sklearn-fraud-process-2021-08-02-14-20-04-608/output/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {train_preproc_dir_artifact} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-02 14:24:06     171439 sklearn-fraud-process-2021-08-02-14-20-04-608/output/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {test_preproc_dir_artifact} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
